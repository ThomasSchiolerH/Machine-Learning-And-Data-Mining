{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Regression part b\n",
    "### Training and Error calculation"
   ],
   "id": "9f8840457b3c2501"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T10:49:43.868169Z",
     "start_time": "2024-11-12T10:49:43.793685Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# clean and standardize data\n",
    "data = pd.read_csv('../Data/diamonds.csv')\n",
    "\n",
    "# Remove rows where 'x', 'y', or 'z' is 0\n",
    "numerical_columns = ['carat', 'depth', 'table', 'price', 'x', 'y', 'z']\n",
    "diamonds_data = data[(data['x'] != 0) & (data['y'] != 0) & (data['z'] != 0)]\n",
    "\n",
    "# Calculate means and standard deviations for standardization\n",
    "means = diamonds_data[numerical_columns].mean()\n",
    "stds = diamonds_data[numerical_columns].std()\n",
    "\n",
    "# Standardize the numerical columns\n",
    "diamonds_data_standardized = diamonds_data[numerical_columns].copy()\n",
    "for column in numerical_columns:\n",
    "    diamonds_data_standardized[column] = (diamonds_data[column] - means[column]) / stds[column]\n",
    "print(diamonds_data_standardized.head())"
   ],
   "id": "901fd81191379109",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      carat     depth     table     price         x         y         z\n",
      "0 -1.198193 -0.174201 -1.099715 -0.904123 -1.591558 -1.539205 -1.580069\n",
      "1 -1.240405 -1.361078  1.585973 -0.904123 -1.645157 -1.661998 -1.750880\n",
      "2 -1.198193 -3.385749  3.376432 -0.903873 -1.502227 -1.460266 -1.750880\n",
      "3 -1.071556  0.454145  0.243129 -0.902117 -1.368229 -1.319931 -1.295384\n",
      "4 -1.029344  1.082491  0.243129 -0.901866 -1.243165 -1.214679 -1.124573\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T11:02:36.305459Z",
     "start_time": "2024-11-12T10:51:01.936720Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Extract features and target variable\n",
    "X = diamonds_data_standardized.drop(columns=['price']).values\n",
    "y = diamonds_data_standardized['price'].values\n",
    "\n",
    "# Set up parameters for models\n",
    "# ==============================\n",
    "# Define the range for the regularization parameter lambda \n",
    "lambda_range = np.logspace(-4, 4, 10)\n",
    "\n",
    "# Define the range for the number of hidden layers in ANN\n",
    "hidden_layer_options = [(h,) for h in [1, 2, 3, 4]]\n",
    "\n",
    "# Two-Level Cross-Validation\n",
    "# ===========================\n",
    "# Outer fold (K1) and inner fold (K2) set to 10\n",
    "K1 = K2 = 10\n",
    "kf_outer = KFold(n_splits=K1, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize lists to store results\n",
    "results = []\n",
    "\n",
    "# Outer cross-validation loop\n",
    "for train_outer_idx, test_outer_idx in kf_outer.split(X):\n",
    "    # Split data into training and testing sets for the outer fold\n",
    "    X_train_outer, X_test_outer = X[train_outer_idx], X[test_outer_idx]\n",
    "    y_train_outer, y_test_outer = y[train_outer_idx], y[test_outer_idx]\n",
    "\n",
    "    # Inner cross-validation loop for model selection\n",
    "    kf_inner = KFold(n_splits=K2, shuffle=True, random_state=42)\n",
    "    best_ann_model = None\n",
    "    best_ridge_model = None\n",
    "    best_ann_error = float('inf')\n",
    "    best_ridge_error = float('inf')\n",
    "    best_hidden_layer = None\n",
    "    best_lambda = None\n",
    "\n",
    "    # ANN model selection\n",
    "    for hidden_layer in hidden_layer_options:\n",
    "        inner_errors = []\n",
    "        for train_inner_idx, val_inner_idx in kf_inner.split(X_train_outer):\n",
    "            X_train_inner, X_val_inner = X[train_inner_idx], X[val_inner_idx]\n",
    "            y_train_inner, y_val_inner = y[train_inner_idx], y[val_inner_idx]\n",
    "\n",
    "            # Train the ANN model\n",
    "            ann = MLPRegressor(hidden_layer_sizes=hidden_layer, max_iter=1000, random_state=42)\n",
    "            ann.fit(X_train_inner, y_train_inner)\n",
    "\n",
    "            # Predict and calculate error\n",
    "            y_val_pred = ann.predict(X_val_inner)\n",
    "            error = mean_squared_error(y_val_inner, y_val_pred)\n",
    "            inner_errors.append(error)\n",
    "        \n",
    "        # Calculate the average validation error\n",
    "        avg_error = np.mean(inner_errors)\n",
    "        if avg_error < best_ann_error:\n",
    "            best_ann_error = avg_error\n",
    "            best_hidden_layer = hidden_layer\n",
    "            best_ann_model = ann\n",
    "\n",
    "    # Ridge Regression model selection\n",
    "    for lmbda in lambda_range:\n",
    "        inner_errors = []\n",
    "        for train_inner_idx, val_inner_idx in kf_inner.split(X_train_outer):\n",
    "            X_train_inner, X_val_inner = X[train_inner_idx], X[val_inner_idx]\n",
    "            y_train_inner, y_val_inner = y[train_inner_idx], y[val_inner_idx]\n",
    "\n",
    "            # Train the Ridge Regression model\n",
    "            ridge = Ridge(alpha=lmbda)\n",
    "            ridge.fit(X_train_inner, y_train_inner)\n",
    "\n",
    "            # Predict and calculate error\n",
    "            y_val_pred = ridge.predict(X_val_inner)\n",
    "            error = mean_squared_error(y_val_inner, y_val_pred)\n",
    "            inner_errors.append(error)\n",
    "        \n",
    "        # Calculate the average validation error\n",
    "        avg_error = np.mean(inner_errors)\n",
    "        if avg_error < best_ridge_error:\n",
    "            best_ridge_error = avg_error\n",
    "            best_lambda = lmbda\n",
    "            best_ridge_model = ridge\n",
    "\n",
    "    # Baseline model: Predicting the mean of y_train_outer\n",
    "    y_train_mean = np.mean(y_train_outer)\n",
    "    y_baseline_pred = np.full_like(y_test_outer, y_train_mean)\n",
    "    baseline_error = mean_squared_error(y_test_outer, y_baseline_pred)\n",
    "\n",
    "    # Test the best models on the outer test set\n",
    "    y_ann_pred = best_ann_model.predict(X_test_outer)\n",
    "    ann_test_error = mean_squared_error(y_test_outer, y_ann_pred)\n",
    "\n",
    "    y_ridge_pred = best_ridge_model.predict(X_test_outer)\n",
    "    ridge_test_error = mean_squared_error(y_test_outer, y_ridge_pred)\n",
    "\n",
    "    # Store the results for this fold\n",
    "    results.append({\n",
    "        'fold': len(results) + 1,\n",
    "        'best_hidden_layer': best_hidden_layer,\n",
    "        'ann_test_error': ann_test_error,\n",
    "        'best_lambda': best_lambda,\n",
    "        'ridge_test_error': ridge_test_error,\n",
    "        'baseline_error': baseline_error\n",
    "    })\n",
    "\n",
    "# Print results\n",
    "# ==============\n",
    "print(\"Fold | Hidden Layer | ANN Test Error | Lambda | Ridge Test Error | Baseline Error\")\n",
    "for result in results:\n",
    "    print(f\"{result['fold']:>4} | {result['best_hidden_layer'][0]:>12} | {result['ann_test_error']:.4f} | {result['best_lambda']:.4e} | {result['ridge_test_error']:.4f} | {result['baseline_error']:.4f}\")\n",
    "\n",
    "# Conclusion: Compare ANN, Ridge, and Baseline\n",
    "# =============================================\n",
    "ann_errors = [result['ann_test_error'] for result in results]\n",
    "ridge_errors = [result['ridge_test_error'] for result in results]\n",
    "baseline_errors = [result['baseline_error'] for result in results]\n",
    "\n",
    "mean_ann_error = np.mean(ann_errors)\n",
    "mean_ridge_error = np.mean(ridge_errors)\n",
    "mean_baseline_error = np.mean(baseline_errors)\n",
    "\n",
    "print(\"\\nAverage Errors:\")\n",
    "print(f\"ANN: {mean_ann_error:.4f}\")\n",
    "print(f\"Ridge: {mean_ridge_error:.4f}\")\n",
    "print(f\"Baseline: {mean_baseline_error:.4f}\")\n"
   ],
   "id": "5ce31f074abecec",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold | Hidden Layer | ANN Test Error | Lambda | Ridge Test Error | Baseline Error\n",
      "   1 |            2 | 0.1217 | 1.6681e+02 | 0.1256 | 0.9676\n",
      "   2 |            2 | 0.1293 | 1.6681e+02 | 0.1444 | 1.0494\n",
      "   3 |            2 | 0.1205 | 1.6681e+02 | 0.1319 | 0.9263\n",
      "   4 |            2 | 0.1250 | 1.6681e+02 | 0.1521 | 1.0162\n",
      "   5 |            2 | 0.1288 | 1.6681e+02 | 0.1409 | 1.0044\n",
      "   6 |            2 | 0.1181 | 1.6681e+02 | 0.1380 | 1.0204\n",
      "   7 |            2 | 0.1355 | 1.6681e+02 | 0.1495 | 0.9810\n",
      "   8 |            2 | 0.1337 | 1.6681e+02 | 0.1485 | 1.0489\n",
      "   9 |            2 | 0.1232 | 1.6681e+02 | 0.1345 | 0.9413\n",
      "  10 |            2 | 0.1543 | 1.6681e+02 | 0.1583 | 1.0451\n",
      "\n",
      "Average Errors:\n",
      "ANN: 0.1290\n",
      "Ridge: 0.1424\n",
      "Baseline: 1.0001\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### T-test",
   "id": "c566d0f50ef56003"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T11:25:34.039965Z",
     "start_time": "2024-11-12T11:25:34.019333Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Errors from each model based on the cross-validation results\n",
    "ann_errors = np.array([0.1217, 0.1293, 0.1205, 0.1250, 0.1288, 0.1181, 0.1355, 0.1337, 0.1232, 0.1543])\n",
    "linear_regression_errors = np.array([0.1256, 0.1444, 0.1319, 0.1521, 0.1409, 0.1380, 0.1495, 0.1485, 0.1345, 0.1583])\n",
    "baseline_errors = np.array([0.9676, 1.0494, 0.9263, 1.0162, 1.0044, 1.0204, 0.9810, 1.0489, 0.9413, 1.0451])\n",
    "\n",
    "# Function to perform paired t-test and calculate confidence interval\n",
    "def paired_t_test_and_ci(errors1, errors2, alpha=0.05):\n",
    "    # Perform paired t-test\n",
    "    t_stat, p_value = stats.ttest_rel(errors1, errors2)\n",
    "    \n",
    "    # Calculate mean difference and standard error of the difference\n",
    "    differences = errors1 - errors2\n",
    "    mean_diff = np.mean(differences)\n",
    "    se_diff = stats.sem(differences)\n",
    "    \n",
    "    # Calculate confidence interval\n",
    "    confidence_interval = (mean_diff - stats.t.ppf(1 - alpha / 2, df=len(differences) - 1) * se_diff,\n",
    "                          mean_diff + stats.t.ppf(1 - alpha / 2, df=len(differences) - 1) * se_diff)\n",
    "    \n",
    "    return t_stat, p_value, confidence_interval\n",
    "\n",
    "# Pairwise comparisons\n",
    "# 1. ANN vs Linear Regression\n",
    "t_stat_ann_lr, p_value_ann_lr, ci_ann_lr = paired_t_test_and_ci(ann_errors, linear_regression_errors)\n",
    "\n",
    "# 2. ANN vs Baseline\n",
    "t_stat_ann_baseline, p_value_ann_baseline, ci_ann_baseline = paired_t_test_and_ci(ann_errors, baseline_errors)\n",
    "\n",
    "# 3. Linear Regression vs Baseline\n",
    "t_stat_lr_baseline, p_value_lr_baseline, ci_lr_baseline = paired_t_test_and_ci(linear_regression_errors, baseline_errors)\n",
    "\n",
    "# Print results\n",
    "print(\"Pairwise t-test Results:\\n\")\n",
    "print(f\"ANN vs Linear Regression:\\n  t-statistic = {t_stat_ann_lr:.4f}, p-value = {p_value_ann_lr:.4e}, 95% CI = {ci_ann_lr}\\n\")\n",
    "print(f\"ANN vs Baseline:\\n  t-statistic = {t_stat_ann_baseline:.4f}, p-value = {p_value_ann_baseline:.4e}, 95% CI = {ci_ann_baseline}\\n\")\n",
    "print(f\"Linear Regression vs Baseline:\\n  t-statistic = {t_stat_lr_baseline:.4f}, p-value = {p_value_lr_baseline:.4e}, 95% CI = {ci_lr_baseline}\\n\")\n",
    "\n",
    "# Conclusion based on p-values and confidence intervals\n",
    "if p_value_ann_lr < 0.05:\n",
    "    print(\"There is a significant difference between ANN and Linear Regression.\")\n",
    "else:\n",
    "    print(\"There is no significant difference between ANN and Linear Regression.\")\n",
    "\n",
    "if p_value_ann_baseline < 0.05:\n",
    "    print(\"ANN significantly outperforms the Baseline model.\")\n",
    "else:\n",
    "    print(\"There is no significant difference between ANN and the Baseline model.\")\n",
    "\n",
    "if p_value_lr_baseline < 0.05:\n",
    "    print(\"Linear Regression significantly outperforms the Baseline model.\")\n",
    "else:\n",
    "    print(\"There is no significant difference between Linear Regression and the Baseline model.\")\n"
   ],
   "id": "9d88c1046df70505",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pairwise t-test Results:\n",
      "\n",
      "ANN vs Linear Regression:\n",
      "  t-statistic = -6.1564, p-value = 1.6743e-04, 95% CI = (np.float64(-0.018269130071275473), np.float64(-0.008450869928724526))\n",
      "\n",
      "ANN vs Baseline:\n",
      "  t-statistic = -68.6364, p-value = 1.4945e-13, 95% CI = (np.float64(-0.8997585728156408), np.float64(-0.8423414271843592))\n",
      "\n",
      "Linear Regression vs Baseline:\n",
      "  t-statistic = -71.0297, p-value = 1.0984e-13, 95% CI = (np.float64(-0.8850057587608299), np.float64(-0.8303742412391703))\n",
      "\n",
      "There is a significant difference between ANN and Linear Regression.\n",
      "ANN significantly outperforms the Baseline model.\n",
      "Linear Regression significantly outperforms the Baseline model.\n"
     ]
    }
   ],
   "execution_count": 13
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
